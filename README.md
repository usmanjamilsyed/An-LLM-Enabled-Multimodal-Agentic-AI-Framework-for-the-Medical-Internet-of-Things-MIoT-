# An LLM-Enabled-Multimodal-Agentic-AI Framework for the Medical Internet of Things MIoT
Integration of large language models (LLM) with multimodal agentic artificial intelligence (AI) within the medical Internet of Things (MIoT) ecosystem

Authors detail:  M. A. RAHMANâˆ—, College of Computer and Cyber Sciences, University of Prince Mugrin, Madina, KSA
SYED USMAN JAMIL, School of Computing, Mathematics and Engineering, Charles Sturt University, Wagga
Wagga NSW, Australia
M SHAMIM HOSSAIN, Department of Software Engineering, College of Computer and Information Sciences,
King Saud University, KSA
MUHAMMAD ARIF KHAN, School of Computing, Mathematics and Engineering, Charles Sturt University, Wagga
Wagga NSW, Australia
TANVEER ZIA, School of Arts and Sciences, University of Notre Dame, NSW, Australia
MUHAMMAD ALI PARACHA, CDC, Melbourne, Australia
MUBARAK ALRASHOUD, Department of Software Engineering, College of Computer and Information Sciences,
King Saud University, KSA
MIN CHEN, School of Computer Science and Engineering, South China University of Technology, China
SELWA AL-HAZZAA, Digital Innovation Sector, King Abdulaziz City for Science and Technology (KACST), KSA

**Introduction**

This repository explores the integration of Large Language Models (LLMs) and multimodal, agentic AI within the Medical Internet of Things (MIoT), a synergy that is fundamentally reshaping modern healthcare. By combining diverse data streams, this approach enables continuous patient monitoring, adaptive clinical decision support, and intelligent, context-aware human-machine collaboration.
Healthcare environments produce vast amounts of multimodal data: text from electronic health records (EHRs) and clinical notes; medical scans like MRI, CT, and X-rays; audio from patient consultations; video for therapy monitoring; and real-time sensor data such as ECG and glucose levels. Traditional single-mode AI struggles to interpret this complex ecosystem. In contrast, LLM-augmented agentic systems excel at fusing these varied sources, grounding their reasoning in medical knowledge, and orchestrating specialized agents to improve real-world clinical operations.
We provide a comprehensive guide to LLM-driven multimodal agentic AI for MIoT healthcare systems. The core of this work is a six-dimensional unified taxonomy covering:
â€¢	Multimodal Input Channels
â€¢	Data Fusion Techniques
â€¢	Core LLM Reasoning Abilities
â€¢	Agent Coordination Models
â€¢	Computational Deployment Layers
â€¢	Ethical Governance Frameworks

To illustrate this framework in practice, we include a detailed Virtual Hospital case study focused on oncology. This study demonstrates how intelligent agents integrate multimodal signals such as medical images, genomic data, patient conversations, and clinical updates to deliver personalized diagnostics, automate documentation, support home-based rehabilitation, and facilitate rapid emergency response.

**Contact** ðŸ“¬: If you want to contribute, you can contact:  [Syed Usman Jamil](usmanjamilsyed@gmail.com) ðŸ“§ Prof. M SHAMIM HOSSAIN

**Contributions:**

A novel taxonomy that structures multimodal agentic AI powered by LLM in MIoT healthcare across six layers:

Layer 1: Input multimedia streams

Layer 2: Fusion strategies

Layer 3: LLM roles 

Layer 4: Agentic and multi-agent coordination

Layer 5: Deployment architectures

Layer 6: Governance/compliance

**Datasets and Benchmarks:** Publicly Available MIoT and multimodal healthcare datasets and highlits of benchmark initiatives

Physiological Signals (Time Series)
MIMIC-IV [1]	ICU data with ECG, vitals, labs, and clinical notes
PhysioNet Databases	Wide range of ECG, EEG, EMG, and PPG datasets
WESAD	Wearable stress and affect recognition dataset (multi-sensor)
Medical Imaging
NIH ChestX-ray14 [2], CheXpert	Large-scale radiology image-label datasets
LIDC-IDRI	Lung CT scans for tumor detection
EyePACS, Messidor	Fundus images for diabetic retinopathy
HAM10000	Dermoscopy images for skin cancer detection
Audio and Speech
IEMOCAP [3]	Emotion recognition, clinically relevant for PROM data
COUGHVID	Crowdsourced dataset of cough sounds for respiratory disease
Heart and Lung Sound Databases	PhysioNetâ€™s auscultation datasets
Video and Multimodal Action Monitoring
UTD-MHAD [4]	Wearable + video dataset for human activity recognition
UP-Fall Detection	Video + accelerometer dataset for fall recognition
MM-Fit	Multimodal rehab exercise dataset (video + IMU)
Textual Data (EHR, Clinical Notes)
MIMIC-IV Notes [5]	De-identified ICU notes
i2b2 2014 De-identification Challenge	Annotated clinical narratives
n2c2 Shared Tasks	Clinical NLP datasets covering discharge summaries, medications, temporal relations
Multimodal Composite Sets
Med-VQA datasets [6]	Image + text question pairs
AVEC	Audioâ€“videoâ€“text multimodal affect recognition challenge datasets
BioVid Heat Pain Database	Physiological + video for pain estimation


